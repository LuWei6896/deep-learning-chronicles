{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.Requests import Requests\n",
    "from utils.helpers import parse_dates\n",
    "from bs4 import BeautifulSoup as BS\n",
    "\n",
    "def get_arxiv(url):\n",
    "    r = Requests()\n",
    "    text = r.get(url)\n",
    "    soup = BS(text, 'html.parser')\n",
    "    \n",
    "    title = soup.find('meta', {'name': 'citation_title'})['content']\n",
    "    authors = [a.text for a in soup.find('div', {'class': 'authors'}).find_all('a')]\n",
    "    dates = soup.find('div', {'class': 'dateline'}).text.split(' (v1), ')\n",
    "    dates = [date.replace('(Submitted on ', '').replace('last revised ', '').split(' (')[0].strip(' ()') for date in dates]\n",
    "    dates = parse_dates(dates)\n",
    "    abstract = soup.find('blockquote', {'class': 'abstract mathjax'}).text.replace('Abstract: ', '').replace('\\n', ' ').strip()\n",
    "    \n",
    "    result = {\n",
    "        'title': title, \n",
    "        'authors': authors, \n",
    "        'dates': dates,\n",
    "        'abstract': abstract\n",
    "    }\n",
    "    return result\n",
    "\n",
    "def get_acm(url):\n",
    "    r = Requests()\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/12.0 Safari/605.1.15'}\n",
    "    text = r.get(url, params={'headers': headers})\n",
    "    soup = BS(text, 'html.parser')\n",
    "    \n",
    "    title = soup.find('title').text\n",
    "    authors = [a.text for a in soup.find_all('table')[4].find('table').find_all('table')[1].find_all('a')[::2]]\n",
    "    date_tokens = soup.find('td', {'style': 'padding-left:10px; padding-bottom:10px'}).text.split('\\n')[3].split(' â€” ')[1].split()\n",
    "    dates = parse_dates([' '.join([date_tokens[0], date_tokens[1], date_tokens[4]])])\n",
    "    abstract = r.get(\n",
    "        'https://dl.acm.org/tab_abstract.cfm?id={}&type=Article&usebody=tabbody&_cf_containerId=cf_layoutareaabstract&_cf_nodebug=true&_cf_nocache=true&_cf_rc=0'.format(url.split('=')[1]), \n",
    "        params={'headers': headers}\n",
    "    )\n",
    "    abstract = BS(abstract, 'html.parser').find('p').text\n",
    "    \n",
    "    result = {\n",
    "        'title': title, \n",
    "        'authors': authors, \n",
    "        'dates': dates,\n",
    "        'abstract': abstract\n",
    "    }\n",
    "    return result\n",
    "\n",
    "def get_acl(url):\n",
    "    r = Requests()\n",
    "    text = r.get(url)\n",
    "    soup = BS(text, 'html.parser')\n",
    "    \n",
    "    title = soup.find('meta', {'name': 'citation_title'})['content']\n",
    "    authors = [m['content'] for m in soup.find_all('meta', {'name': 'citation_author'})]\n",
    "    dates = parse_dates([soup.find('meta', {'name': 'citation_publication_date'})['content']], reset=True)\n",
    "    abstract = ''\n",
    "    \n",
    "    result = {\n",
    "        'title': title, \n",
    "        'authors': authors, \n",
    "        'dates': dates,\n",
    "        'abstract': abstract\n",
    "    }\n",
    "    return result\n",
    "\n",
    "def get_scholarpedia(url):\n",
    "    r = Requests()\n",
    "    text = r.get(url)\n",
    "    soup = BS(text, 'html.parser')\n",
    "    \n",
    "    title = soup.find('meta', {'name': 'citation_title'})['content']\n",
    "    authors = [m['content'] for m in soup.find_all('meta', {'name': 'citation_author'})]\n",
    "    dates = parse_dates([soup.find('meta', {'name': 'citation_date'})['content']])\n",
    "    abstract = ''\n",
    "    \n",
    "    result = {\n",
    "        'title': title, \n",
    "        'authors': authors, \n",
    "        'dates': dates,\n",
    "        'abstract': abstract\n",
    "    }\n",
    "    return result\n",
    "\n",
    "def get_nature(url):\n",
    "    r = Requests()\n",
    "    text = r.get(url)\n",
    "    soup = BS(text, 'html.parser')\n",
    "    \n",
    "    title = soup.find('meta', {'name': 'citation_title'})['content']\n",
    "    authors = [m['content'] for m in soup.find_all('meta', {'name': 'citation_author'})]\n",
    "    dates = parse_dates([soup.find('meta', {'name': 'citation_online_date'})['content']])\n",
    "    abstract = soup.find('div', {'id': 'abstract-content'}).find('p').text\n",
    "    \n",
    "    result = {\n",
    "        'title': title, \n",
    "        'authors': authors, \n",
    "        'dates': dates,\n",
    "        'abstract': abstract\n",
    "    }\n",
    "    return result\n",
    "\n",
    "def get_empty(url):\n",
    "    result = {\n",
    "        'title': '', \n",
    "        'authors': '', \n",
    "        'dates': '',\n",
    "        'abstract': ''\n",
    "    }\n",
    "    return result\n",
    "\n",
    "# r = Requests()\n",
    "# url = 'https://arxiv.org/abs/1612.03242'\n",
    "# text = r.get(url)\n",
    "# soup = BS(text, 'html.parser')\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for idx, row in df.iterrows():\n",
    "    url = row['url']\n",
    "    info = {\n",
    "        'name': row['name'], \n",
    "        'source': row['source'], \n",
    "        'url': row['url']\n",
    "    }\n",
    "    if 'arxiv' in url:\n",
    "        info.update(get_arxiv(url))\n",
    "    if 'acm' in url:\n",
    "        info.update(get_acm(url))\n",
    "    if 'acl' in url:\n",
    "        info.update(get_acl(url))\n",
    "    if 'scholarpedia' in url:\n",
    "        info.update(get_scholarpedia(url))\n",
    "    if 'nature' in url:\n",
    "        info.update(get_nature(url))\n",
    "    result.append(info)\n",
    "print(result[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('works.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
